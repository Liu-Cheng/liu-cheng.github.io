(window.webpackJsonp=window.webpackJsonp||[]).push([[9],{210:function(e,t,a){"use strict";var i=a(74);a.n(i).a},220:function(e,t,a){"use strict";a.r(t);a(210);var i=a(0),r=Object(i.a)({},(function(){var e=this,t=e.$createElement,a=e._self._c||t;return a("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[a("h2",{attrs:{id:"projects"}},[e._v("Projects")]),e._v(" "),a("ProjectCard",[a("p",[a("strong",[e._v("TaijiGraph: Large-scale graph processing accelerator and system")]),e._v(" (Ongoing)")]),e._v(" "),a("p",[e._v("By offering in-storage computing facilities with massive bandwidth to the stored data, computational storage devices (CSDs) promise to address the I/O bottleneck of out-of-core graph processing systems that operate on large graphs.\nUnfortunately, despite their I/O advantages, the in-storage computing capability as well as the memory capacity of existing CSDs can be up to two orders of magnitude lower than the host system, shifting the bottleneck from I/O to computing if CSDs alone were used to process large graphs. To address this problem, we propose TaijiGraph, which combines the weak computing engine inside a CSD that has high I/O bandwidth with powerful host processors that have limited I/O bandwidth to achieve high overall performance for out-of-core graph processing.\nTaijiGraph leverages the strengths of the two computing systems by partitioning the input graph according to the degree of a vertex and dynamically assigning workloads to the host and the CSD subsequently. High-degree vertices, which typically result in a higher compute-to-memory ratio in many graph kernels, are processed by the host, while low-degree vertices that typically result in a lower compute-to-memory ratio are processed by the CSD. In addition, a linear performance predictor that depends on the number of active vertices and edges is developed to capture the dynamics of the graph processing tasks across the partitions and iterations.\nUsing this performance predictor, a lightweight runtime scheduler that dynamically offloads I/O-intensive graph processing tasks to CSD while leaving computing-intensive tasks on the host is proposed. On a system using a Cosmos+ OpenSSD prototype with a 30X weaker in-storage processor than the host, when compared to GraphChi and GridGraph implemented on the same system using a wide range of large graphs, TaijiGraph achieves an average performance speedup of 6.41X and 2.62X while reducing data transmission between the host and the CSD by 13.94X and 6.52X respectively, making it promising for use in future disaggregated computing and storage systems.")]),e._v(" "),a("p",[e._v("["),a("a",{attrs:{href:"https://anonymous.4open.science/r/TaijiGraph2022",target:"_blank",rel:"noopener noreferrer"}},[e._v("TaijiGraph on Computational Storage"),a("OutboundLink")],1),e._v("]")])]),e._v(" "),a("ProjectCard",[a("p",[a("strong",[e._v("RobustDL: Robust deep learning toolchain")]),e._v(" (Ongoing)")]),e._v(" "),a("p",[e._v("We mainly investigate the design of deep learning toolchains all the way from deep learning applications to low-level circuit designs. Specifically, we explore model-independent fault-tolerant algorithm design, model-dependent fault-tolerant algorithm design, fault-tolerant compilation, fault-tolerant architecture design, fault-tolerant circuit design, and cross-layer fault-tolerant designs. In addition, we also develop a set of fault injection tools from different perspectives to assist fault-tolerant design at different abstraction levels.")]),e._v(" "),a("p",[e._v("Neural network accelerators are increasingly used because of their competitive advantages in performance and energy efficiency. Their reliability is critical to these applications, and must be comprehensively evaluated and verified to ensure application safety. There are different requirements for the accuracy of reliability analysis at different stages of design. It will be very time-consuming to rely only on the very accurate reliability analysis at the bottom level, and may get biased results due to the limited number of fault simulations. Simulation at a higher level can provide reliability analysis more quickly and provide more extensive conclusions, but it can be inacurate because of the lack of the underlying hardware architecture details. Therefore, we provide multi-level fault analysis tools or methods for different scenarios, including "),a("strong",[e._v("FPGA-based reliability emulation")]),e._v(", "),a("strong",[e._v("operation level fault injection")]),e._v(", "),a("strong",[e._v("neuron level fault injection")]),e._v(", "),a("strong",[e._v("statistical model based reliability analysis")]),e._v(". In addition, we also made "),a("strong",[e._v("application level fault injection and analysis")]),e._v(" specifically for autonomous driving scenarios.")]),e._v(" "),a("p",[e._v("["),a("a",{attrs:{href:"https://github.com/fffasttime/MR-Neural-Network-Reliability-Analysis-Toolbox",target:"_blank",rel:"noopener noreferrer"}},[e._v("Multi-Resolution Fault Injection Tools"),a("OutboundLink")],1),e._v("]")])]),e._v(" "),a("ProjectCard",[a("p",[a("strong",[e._v("AutoDSA: Automatic domain-specific accelerator generation (Ongoing)")])]),e._v(" "),a("p",[e._v("We investigate DSA design automation approaches from different perspectives including HLS template based frameworks, hardware overlays, and domain specific languages. In order to optimize the resulting DSAs, DSE tools are also explored and integrated.")]),e._v(" "),a("p",[e._v("Mixed-precision neural networks (MPNNs) that enable the use of just enough data width for a deep learning task promise significant advantages of both inference accuracy and computing overhead. FPGAs with fine-grained reconfiguration capability can adapt the processing with distinct data width and models, and hence, can theoretically unleash the potential of MPNNs. Nevertheless, commodity DPUs on FPGAs mostly emphasize the generality and have limited support for MPNNs especially the ones with lower data width. In addition, primitive DSPs in FPGAs usually have much larger data width than that is required by MPNNs and havenâ€™t been sufficiently co-explored with MPNNs yet. To this end, we propose an open source MPNN accelerator design framework specifically tailored for FPGAs. In this framework, we have a systematic DSP-packing algorithm to pack multiple lower data width MACs in a single primitive DSP and enable efficient implementation of MPNNs. Meanwhile, we take the DSP-packing efficiency into consideration with MPNN quantization within a unified neural network architecture search (NAS) framework such that it can be aware of the DSP overhead during quantization and optimize the MPNN performance and accuracy concurrently. Finally, we have the optimized MPNN fine-tuned to a fully pipelined neural network accelerator template based on HLS and make best use of available resources for higher performance. Our experiments reveal the resulting accelerators produced by the proposed framework can achieve overwhelming advantages in terms of performance, resource utilization, and inference accuracy for MPNNs when compared with both hand-crafted counterparts and prior hardware-aware neural network accelerators on FPGAs.")]),e._v(" "),a("p",[e._v("["),a("a",{attrs:{href:"https://anonymous.4open.science/r/NASPacking-8B1F/",target:"_blank",rel:"noopener noreferrer"}},[e._v("Mixed Precision Neural Network Acceleration on FPGAs"),a("OutboundLink")],1),e._v("]")]),e._v(" "),a("p",[e._v("Breadth First Search (BFS) is a key building block of graph processing and there have been considerable efforts devoted to accelerating BFS on FPGAs for both performance and energy efficiency. Prior work typically built the BFS accelerator through handcrafted circuit design using hardware description language (HDL). Despite the relatively good performance, the HDL based design leads to extremely low design productivity, and incurs high portability and maintenance cost. While high level synthesis (HLS)  tools make it convenient to create a functionally correct BFS accelerator, the performance can be much lower the handcrafted design with HDL. To obtain both the near handcrafted design performance and better software-like features such as portability and maintenance, we propose OBFS, an OpenCL based BFS accelerator on software programmable FPGAs. With the observation that OpenCL based FPGA design is rather inefficient on irregular memory accesses, we propose approaches including data alignment, graph reordering and batching to ensure coalesced memory accesses. In addition, we take advantage of the on-chip buffer to mitigate the inefficient random DDR accesses. Finally, we shift the random level update in BFS out from the main processing pipeline and have it overlapped with the following BFS processing task. According to the experiments, OBFS achieves 9.5X and 5.5X performance speedup on average compared to a vertex-centric implementation and an edge-centric implementation respectively on Intel Harp-v2. When compared to prior handcrafted designs, it achieves comparable or even better performance.")]),e._v(" "),a("p",[e._v("["),a("a",{attrs:{href:"https://github.com/Liu-Cheng/bfs_with_Intel_OpenCL.git",target:"_blank",rel:"noopener noreferrer"}},[e._v("BFS Acceleration with Intel OpenCL"),a("OutboundLink")],1),e._v("]")])]),e._v(" "),a("ProjectCard",[a("p",[a("strong",[e._v("DeepBurning: Automatic generation of FPGA-based learning accelerators for the neural network family")]),e._v(" (2018-2020)")]),e._v(" "),a("p",[e._v("DeepBurning is an end-to-end automatic neural network accelerator design tool for specialized learning tasks. It provides a unified deep learning acceleration solution to high-level application designers without dealing with the model training and hardware accelerator tuning. You can refer to DeepBurning homepage for more details.")]),e._v(" "),a("p",[e._v("["),a("a",{attrs:{href:"https://github.com/groupsada/DeepBurning",target:"_blank",rel:"noopener noreferrer"}},[e._v("DeepBurning Project"),a("OutboundLink")],1),e._v("]")])]),e._v(" "),a("ProjectCard",[a("p",[a("strong",[e._v("QuickDough: A rapid loop acceleration on closely coupled CPU-FPGA architectures")]),e._v(" (2011-2016)")]),e._v(" "),a("p",[e._v("QuickDough is developed to address the FPGA design productivity problem. By utilizing a soft coarse-grained reconfigurable array (SCGRA) overlay built on top of off-the-shelf FPGAs, it compiles a high-level loop to the overlay through a rapid operation scheduling first and then generates the FPGA accelerator bitstream through a rapid integration of the scheduling result and a pre-built overlay bitstream.")]),e._v(" "),a("p",[e._v("["),a("a",{attrs:{href:"https://github.com/Liu-Cheng/QuickDough",target:"_blank",rel:"noopener noreferrer"}},[e._v("QuickDough Project"),a("OutboundLink")],1),e._v("]")])])],1)}),[],!1,null,null,null);t.default=r.exports},74:function(e,t,a){}}]);