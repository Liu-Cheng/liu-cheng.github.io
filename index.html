<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Cheng Liu</title>
    <meta name="description" content="Welcome to Cheng Liu&#39;s Homepage.">
    <link rel="icon" href="/logo.png">
    
    <link rel="preload" href="/assets/css/0.styles.992c4117.css" as="style"><link rel="preload" href="/assets/js/app.68ae2154.js" as="script"><link rel="preload" href="/assets/js/2.8f7e0c52.js" as="script"><link rel="preload" href="/assets/js/7.1f22d049.js" as="script"><link rel="preload" href="/assets/js/5.96dea912.js" as="script"><link rel="prefetch" href="/assets/js/10.11606e59.js"><link rel="prefetch" href="/assets/js/11.bcb96f38.js"><link rel="prefetch" href="/assets/js/3.24943c52.js"><link rel="prefetch" href="/assets/js/4.49eae7c6.js"><link rel="prefetch" href="/assets/js/6.f5871937.js"><link rel="prefetch" href="/assets/js/8.e8151f97.js"><link rel="prefetch" href="/assets/js/9.7e14577c.js">
    <link rel="stylesheet" href="/assets/css/0.styles.992c4117.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container no-sidebar home-page"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-exact-active router-link-active"><!----> <span class="site-name">Cheng Liu</span></a> <div class="links"><!----> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link router-link-exact-active router-link-active">Home</a></div><div class="nav-item"><a href="/projects/" class="nav-link">Projects</a></div><div class="nav-item"><a href="/article/" class="nav-link">Publications</a></div><div class="nav-item"><a href="https://github.com/Liu-Cheng" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link router-link-exact-active router-link-active">Home</a></div><div class="nav-item"><a href="/projects/" class="nav-link">Projects</a></div><div class="nav-item"><a href="/article/" class="nav-link">Publications</a></div><div class="nav-item"><a href="https://github.com/Liu-Cheng" target="_blank" rel="noopener noreferrer" class="nav-link external">
  GitHub
  <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a></div> <!----></nav>  <!----> </aside> <main class="page"> <div class="theme-default-content content__default"><div class="profile"><div class="image"><img src="/profile.jpg" alt></div> <div class="info"><div class="name">
      Cheng Liu
    </div> <div class="bio"><p></p></div> <div class="socials"><div><a href="https://github.com/Liu-Cheng" target="_blank"><img src="/icons/github.svg" alt="github" title="github"></a></div><div><a href="https://www.linkedin.com/feed/?trk=eml-wym-cta" target="_blank"><img src="/icons/linkedin-mono.svg" alt="linkedin" title="linkedin"></a></div><div><a href="https://scholar.google.com/citations?user=LVWU_VQAAAAJ&amp;hl=en" target="_blank"><img src="/icons/googlescholar.svg" alt="googlescholar" title="googlescholar"></a></div></div> <div class="contact"><div title="Contact me" class="email">liucheng (at) ict (dot) ac (dot) cn</div></div> <!----></div></div> <h2 id="about-me">About Me</h2> <p>I am an associate professor in State Key Laboratory of Processors (SKLP), Institute of Computing Technology (ICT), and work in IC design group led by Prof. Xiaowei Li and Prof. Huawei Li. I got my B.Eng degree and M.Eng degree from Harbin Institute of Technology in 2007 and 2009 respectively. I got my Ph.D degree from The University of Hong Kong advised by Prof. Hayden So in 2016. I also worked as a research fellow in Xtra group led by Prof. Bingsheng He in National University of Singapore from 2016 to 2018. My research interest includes domain specific architecture and system, in-storage processing, reconfigurable computing, and fault-tolerant computing.</p> <h2 id="vancancies">Vancancies</h2> <p>I am looking for self-motivated intern students. Students with EE and CS background are preferred. For intern students, it is possible to work fully remotely. Topics for interns are listed as follows. If you are interested in my research, contact me with email.</p> <ul><li>AI for circuit design and optimization</li> <li>Automatic neural network accelerator customization on FPGAs</li> <li>Light-weight neural network acceleration on the edge</li> <li>DSL-based graph processing accelerator and system</li> <li>In-storage big data processing systems (graph processing and information retriveal)</li> <li>Reliable AI toolchain development</li></ul> <h2 id="news">News</h2> <ul><li>[June 2020] Shengwen Liang and Rick Lee won the Third Prize (FPGA Track) of the 2020 IEEE Low-Power Computer Vision Challenge (<a href="https://lpcv.ai/" target="_blank" rel="noopener noreferrer">LPCVC<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>).</li> <li>[July 2022] Haitong Huang, Erjing Luo, and Cangyuan Li in my group got the Third Place in DAC'22 SDC.</li> <li>[Jan. 2023] <font color="red"> Our work &quot;EnGN: A High-Throughput and Energy-Efficient Accelerator for Large Graph Neural Networks&quot; won the Best Paper Award of TC'21. </font></li> <li>[Jan. 2023] <font color="red"> Our work &quot;S2Loop: a Lightweight Spectral-Spatio Loop Closure Detector for Resource-Constrained Platforms&quot; is accepted by IEEE Robotics and Automation Letters.</font></li> <li>[Jan. 2023] <font color="red"> Our work &quot;MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-linear Functions&quot; is accepted by ICLR'23</font></li> <li>[Apr. 2023] <font color="red"> Our work &quot;Statistical Modeling of Soft Error Influence on Neural Networks&quot; is accepted by TCAD'23.</font></li> <li>[May 2023] <font color="red"> Our work &quot;Accelerating Deformable Convolution Networks with Dynamic and Irregular Memory Accesses&quot; is accepted by TODAES'23.</font></li> <li>[May 2023] <font color="red"> Our work &quot;Layer-Puzzle: Allocating and Scheduling Multi-Task on Multi-Core NPUs By Using Layer Heterogeneity&quot; is accepted by DATE'23.</font></li> <li>[June 2023] <font color="red"> Congratulations! Haitong Huang, Erjing Luo, and Guoyu Li got into Top 3 in DAC'23 SDC.</font></li></ul> <h2 id="projects">Projects</h2> <ul><li><strong>TaijiGraph: Large-scale graph processing accelerator and system</strong> (Ongoing)</li></ul> <p>By offering in-storage computing facilities with massive bandwidth to the stored data, computational storage devices (CSDs) promise to address the I/O bottleneck of out-of-core graph processing systems that operate on large graphs.
Unfortunately, despite their I/O advantages, the in-storage computing capability as well as the memory capacity of existing CSDs can be up to two orders of magnitude lower than the host system, shifting the bottleneck from I/O to computing if CSDs alone were used to process large graphs. To address this problem, we propose TaijiGraph, which combines the weak computing engine inside a CSD that has high I/O bandwidth with powerful host processors that have limited I/O bandwidth to achieve high overall performance for out-of-core graph processing.
TaijiGraph leverages the strengths of the two computing systems by partitioning the input graph according to the degree of a vertex and dynamically assigning workloads to the host and the CSD subsequently. High-degree vertices, which typically result in a higher compute-to-memory ratio in many graph kernels, are processed by the host, while low-degree vertices that typically result in a lower compute-to-memory ratio are processed by the CSD. In addition, a linear performance predictor that depends on the number of active vertices and edges is developed to capture the dynamics of the graph processing tasks across the partitions and iterations.
Using this performance predictor, a lightweight runtime scheduler that dynamically offloads I/O-intensive graph processing tasks to CSD while leaving computing-intensive tasks on the host is proposed. On a system using a Cosmos+ OpenSSD prototype with a 30X weaker in-storage processor than the host, when compared to GraphChi and GridGraph implemented on the same system using a wide range of large graphs, TaijiGraph achieves an average performance speedup of 6.41X and 2.62X while reducing data transmission between the host and the CSD by 13.94X and 6.52X respectively, making it promising for use in future disaggregated computing and storage systems.</p> <p>[<a href="https://anonymous.4open.science/r/TaijiGraph2022" target="_blank" rel="noopener noreferrer">TaijiGraph on Computational Storage<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p> <ul><li><strong>RobustDL: Robust deep learning toolchain</strong> (Ongoing)</li></ul> <p>We mainly investigate the design of deep learning toolchains all the way from deep learning applications to low-level circuit designs. Specifically, we explore model-independent fault-tolerant algorithm design, model-dependent fault-tolerant algorithm design, fault-tolerant compilation, fault-tolerant architecture design, fault-tolerant circuit design, and cross-layer fault-tolerant designs. In addition, we also develop a set of fault injection tools from different perspectives to assist fault-tolerant design at different abstraction levels.</p> <p>[<a href="https://github.com/fffasttime/MR-Neural-Network-Reliability-Analysis-Toolbox" target="_blank" rel="noopener noreferrer">Multi-Resolution Fault Injection Tools<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p> <ul><li><strong>AutoDSA: Automatic domain-specific accelerator generation (Ongoing)</strong></li></ul> <p>Mixed-precision neural networks (MPNNs) that enable the use of just enough data width for a deep learning task promise significant advantages of both inference accuracy and computing overhead. FPGAs with fine-grained reconfiguration capability can adapt the processing with distinct data width and models, and hence, can theoretically unleash the potential of MPNNs. Nevertheless, commodity DPUs on FPGAs mostly emphasize the generality and have limited support for MPNNs especially the ones with lower data width. In addition, primitive DSPs in FPGAs usually have much larger data width than that is required by MPNNs and haven’t been sufficiently co-explored with MPNNs yet. To this end, we propose an open source MPNN accelerator design framework specifically tailored for FPGAs. In this framework, we have a systematic DSP-packing algorithm to pack multiple lower data width MACs in a single primitive DSP and enable efficient implementation of MPNNs. Meanwhile, we take the DSP-packing efficiency into consideration with MPNN quantization within a unified neural network architecture search (NAS) framework such that it can be aware of the DSP overhead during quantization and optimize the MPNN performance and accuracy concurrently. Finally, we have the optimized MPNN fine-tuned to a fully pipelined neural network accelerator template based on HLS and make best use of available resources for higher performance. Our experiments reveal the resulting accelerators produced by the proposed framework can achieve overwhelming advantages in terms of performance, resource utilization, and inference accuracy for MPNNs when compared with both hand-crafted counterparts and prior hardware-aware neural network accelerators on FPGAs.</p> <p>[<a href="https://anonymous.4open.science/r/NASPacking-8B1F/" target="_blank" rel="noopener noreferrer">Mixed Precision Neural Network Acceleration on FPGAs<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p> <p>[<a href="https://github.com/Liu-Cheng/bfs_with_Intel_OpenCL.git" target="_blank" rel="noopener noreferrer">BFS Acceleration with Intel OpenCL<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p> <ul><li><strong>DeepBurning: Automatic generation of FPGA-based learning accelerators for the neural network family</strong> (2018-2020)</li></ul> <p>DeepBurning is an end-to-end automatic neural network accelerator design tool for specialized learning tasks. It provides a unified deep learning acceleration solution to high-level application designers without dealing with the model training and hardware accelerator tuning. You can refer to DeepBurning homepage for more details.</p> <p>[<a href="https://github.com/groupsada/DeepBurning" target="_blank" rel="noopener noreferrer">DeepBurning Project<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p> <ul><li><strong>QuickDough: A rapid loop acceleration on closely coupled CPU-FPGA architectures</strong> (2011-2016)</li></ul> <p>QuickDough is developed to address the FPGA design productivity problem. By utilizing a soft coarse-grained reconfigurable array (SCGRA) overlay built on top of off-the-shelf FPGAs, it compiles a high-level loop to the overlay through a rapid operation scheduling first and then generates the FPGA accelerator bitstream through a rapid integration of the scheduling result and a pre-built overlay bitstream.</p> <p>[<a href="https://github.com/Liu-Cheng/QuickDough" target="_blank" rel="noopener noreferrer">QuickDough Project<svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg></a>]</p> <h2 id="funding">Funding</h2> <ul><li>Automatic Cross-Layer DSA Design and Optimization, 1 600 000￥, National Key Research and Development Program of China(2023-2025)</li> <li>Intelligent In-Storage Big Data Processing System, 1 300 000￥, 1XX(2023-2024)</li> <li>Fault-tolerant Deep Learning Toolchain for COTS Devices, 300 000￥, XXX(2023)</li> <li>Elastic Fault-tolerant Deep Learning Processor Design, 570 000￥, NSFC(2022-2025)</li> <li>Customized Energy-efficient Graph Processing Acceleration on FPGAs, 300 000￥, NSFC(2020-2022)</li> <li>Fault-tolerant Deep Learning Processor Design Automation, 300 000￥, SKLCA(2021-2022)</li></ul> <h2 id="services">Services</h2> <ul><li>PC for:
<ul><li>DFT'22, FPT'22, ATS'23, FPT'23, DFT'23</li></ul></li> <li>Review for:
<ul><li>TPDS, TCAD, TC, TVLSI, JCST, TETC, JETC, IS</li> <li>ITC'22, ITC'23, NIPS'23</li></ul></li></ul> <h2 id="graduate-students">Graduate Students</h2> <ul><li>Cheng Chu (Intern from Hefei University of Technology, PhD candidate in Indiana University Bloomington)</li> <li>Meng He (Intern from Hefei University of Technology, 字节跳动)</li> <li>Ziyang Zhu (Intern from Hefei University of Technology, 紫光展锐)</li> <li>Qiang Zhang (Master Student, 快手)</li> <li>Kouzi Xing (Intern from Hefei University of Technology, 商汤科技)</li> <li>Li Li (Intern from Hefei University of Technology, 京东)</li> <li>Kexin Chu (Intern from Hefei University of Technology, 百度)</li> <li>Kaijie Tu (Intern from Hefei University of Technology, 计算所)</li> <li>Chang Shi (Master student, Alibaba)</li> <li>Peibin Wu (Master student, MSRA)</li></ul></div> <footer class="page-edit"><!----> <div class="last-updated"><span class="prefix">Last Updated:</span> <span class="time">10/21/2020, 8:22:54 PM</span></div></footer> <!----> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.68ae2154.js" defer></script><script src="/assets/js/2.8f7e0c52.js" defer></script><script src="/assets/js/7.1f22d049.js" defer></script><script src="/assets/js/5.96dea912.js" defer></script>
  </body>
</html>
